How much of the hard, interesting stuff Apple did is with the hardware in the Vision Pro versus the software? Well, you need to understand the real world in order to augment it. The technology of a self-driving car, but on a headset. This is maybe where founders should sort of pay attention. Is this a good opportunity for startups? There are all kinds of new interactions that I think we have not figured out yet. What really, truly takes advantage of this platform? The dream has always been to get to something like this.

Welcome back to another episode of The Light Cone, and as you can see, it’s not just any other day in tech. There are some new platforms that are coming up right now. You might have seen other places where there are reviews. We're not doing reviews today. We're going to talk about what these platforms might mean for founders and people who want to build things for a billion people. We actually have an expert at the table right now, don’t we? We do. Diana, who's a group partner at YC. Before she worked at YC, she's been working in AR and VR for 10 years since the dawn of Oculus. Before VR was a mainstream thing. In fact, her grad school research was in computer vision, so she's been interested in this from way before it was a thing other people were following. 

Diana, do you want to talk about your startup that you did, which was an AR/VR startup, a really early pioneering one? Yeah, we went through YC with a startup called Escher Reality. What we were building was an augmented reality SDK for game developers so that they could build multiplayer experiences and AR games, and build the code once so that it would work on any platform. So between not just IOS and Android mobile devices, but the dream has always been to get to something like this or that or that so that developers would write the code once and work across all devices. 

And what happened to your startup? So what happened is, this took a lot longer to come to market. That's one thing. The other thing that ended up happening, we ended up getting acquired by Niantic, the makers of Pokemon Go. So, I ended up heading a lot of the AR platform over there at Pokemon with Niantic, and we shipped actually a lot of this AR SDK into a lot of games. So millions of players are running our code, which is really cool. So if you've ever played Pokemon Go, you've literally used code that Diana wrote. And I'm so excited with this platform coming in, and we can go dive deeper into it. Okay, should we take their headsets off so we can talk? 

So, it’s been a long road. You’ve seen this technology basically evolve over the course of a decade. What’s, you know, why AR? That’s one of the big things here. Previous platforms may be really focused on VR and the gaming aspect. HoloLens from Microsoft seemed to try to do the AR thing. What’s going on with the Apple Vision Pro? You know, why is this important? Why are we talking about this? Yeah, I mean, we have to go even back in the history of computing. Actually, the attempts of building augmented reality and VR headsets have been actually since the beginning of the first computer. The very first one was by this guy called Ivan Sutherland back in the 60s. People have been thinking about it. It's kind of one of the dreams, and it's one of those things that really fascinated me. 

I think it's so much of it is in our consciousness that we want to make it really happen. But the challenge, why it has not happened unlike tablets, phones, is that it's just really—So you bring up the Microsoft HoloLens. They had version one and version two. And sadly, the latest version got scoped down or the team kind of got let go because they tried an optical approach. So the AR approach was, actually, they were seeing actually the real world, and then the digital content would be rendered just with, uh, within the eyes, and had a very little field of view. It was actually the same approach that Magic Leap was trying. 

And what Apple is trying is actually more of a pass-through, which is actually more of a full high-res video feed of the real world. Arguably, a lot of the technical challenges are a lot easier. And the hard part of optics is that it is not a problem of Moore's Law and just brute forcing with more computation, more pixels. It is actually figuring out new physics and photons. So that they don't have to worry too much about how far they can get into the human eye because the human eye is actually very incredible. Your field of view is actually 210 degrees. So, you put your hands behind your ears, you can kind of see them. And to have a display system that can really render all of that is so hard. And the other part is really hard, which I want to touch upon a bit more, is our eyes have incredible, infinite ability to focus. So we can look close here, far and in some senses, you have to find a way to make that discrete for computers to work right, because computers just understand ones and zeros. To get that working in a display is just so hard and Apple has done some clever things with that, that's different to the optical approach. 

Um, because the optical approach is, what, like—it's actually looking through to the real world, or it’s—what’s the difference? Yeah, so if I'm looking at Jared right now, I'm actually seeing Jared and if I overlay a digital screen—digital information—in the optical system, I would only overlay the digital information and here for the Vision Pro, and what the Meta Quest 3 or Meta Quest Pro or the Vision Pro, technically VR headset, the full video is all digital. Like Jared is technically pixels when I see him through the Vision Pro. And so you said like the Apple Vision Pro being a video feed actually reduces the technical challenge? Yes, because I think, uh, there’s a couple of things you could do. You can play a lot with the video feed and one of the cool things, if you're really the best in the world with display technology, what Apple is, you can get away a lot with it. 

One of the cool things they've done and foundations of what they built, which is actually helpful if you're going to build apps here, so much of it is built upon eye tracking. So they actually have variable rendering for focus. So they had to get eye tracking to be working so well for this to work. So, in the Vision Pro, wherever you look, the pixel density of your focal point will render more high fidelity than where it's not. And the reason why this is important is because to fit it in such a small form factor and not to burn—and there's so much heat dissipation—to push so much pixels and battery, you have to do trade-offs. So they did this thing of rendering more high-res where your eye focuses. So you can notice a little bit in the periphery with the Vision Pro where it's more blurry, or a bit—it's not like quite pixelated but blurry and some other people do complain online with the foveated view. I mean, that's, I think a bit of the artifact with the—with the lenses but that's like a different discussion on how much of like the hard interesting stuff Apple did is with the hardware in the Vision Pro versus the software. I think the cool thing about them is, uh, is both, because the Vision Pro is sort of a culmination of a lot of the ecosystem of what expertise they build in iPhone, like they have custom silicon; they have the R1 processor, which is a co-processor to the M2. The M2 is basically the same processor that runs on the MacBook Pro, so very beefy. But that processor, M2, is for regular kind of like a CPU regular workloads. But the challenge for building an AR headset or AR, in general, you need to understand the real world in order to augment it and for that, you need a lot of sensors. So this has over 10 cameras; it even has a LiDAR, it has a True Depth camera, it has a bunch of IR cameras inside to track your eyes. That's a lot of data, a lot of high data bandwidth that it needs to process. And underneath the hardware, I think this, um, you're gonna get throughput blocked. So the R1 is a custom processor that processes all of the sensor data with very high data channel bandwidth. And I suspect they are even running a real-time operating system along with the Vision OS, which is kind of interesting for what it means for developers to process all of this in real time. And it's starting to sound a lot like actually a technology of a self-driving car but on a headset. 

Yeah, that's exactly as you were talking about what this is, that springs to mind, like LiDAR plus a bunch of cameras and processing the video feed.

From the world of robotics called SLAM (Simultaneous Localization and Mapping), you want to find where a robot is in the world based on just visual data. That is the same thing that self-driving cars look to navigate where they are in the 3D world. You notice in a car there's 3D LiDAR, radars, and a bunch of cameras–same thing here to know where you are in the world. So, it's the same technical challenges, but there’s so much more hardware complexity because you don't want to burn people's heads with this imagined self-driving car. With several cars, you could actually see the actual hardware that runs on self-driving car processing – they put server-grade GPUs and CPUs, which fits in, like, the trunk or underneath.

This is actually pretty cool what they've done, and they built a lot of that because on the iPhone they learned how to build custom processors. They built the TrueFace, True 3D on the camera, which is like IR for mapping in 3D, and LiDAR. They added this on the latest iPads, and they've been building a lot of the ecosystem one by one. It’s interesting here you talk about how Apple can build on their previous products. It's like you're saying this is sort of a lot of the technology here is coming out of the iPhone. This sounds like this sets them up to build their car pretty well, same expertise. 

Let's talk about the use cases a little bit; I mean, one of the things that's pretty clear in everything about the launch of this is it’s focused on productivity. And I kind of like it because, you know, when you're talking about these Oculus devices, they're much more focused on gaming, VR where you're sort of in a totally different place. Whereas you know my guess is one of the reasons why VR AR hadn’t been embraced is that it wasn't something that a busy person would use every single day. Now, though, it's got the M2, the same chip that I have in my MacBook Air. I can actually, with a keyboard, do all of my work all day if I wanted to. That's a really big difference in how they're positioning this device, which is a big departure from Meta, which is so much focused on the gaming community. There was an uproar from the VR community that there are no controllers and Apple has really focused full-on on productivity.

I think this was my dream when we started Escher, that if AR was going to happen, we're not going to notice it because it's going to solve all the very mundane things and it could replace all screens. I think you've done well; this is going after the market cap of all screens that get sold if done well. I mean, there's still a lot of things to be done; this is still B0, but yeah, but this motion is incredibly natural, and being able to look at things and have it be something that you interact with, I was just blown away at how simple, how easy that was to reprogram my brain. This is cool.

I think there's half; I remember, I guess a question for you, Gary, do you remember when the iPhone came out, Apple had this Human Interface Guideline? They had a lot of things about communication and for communicating information hierarchy with touch and focus and gestures with your thumb and things like that. It was an incredibly comprehensive document. They basically took all of the learnings that they had gotten building the iPhone for years and they distilled it into a really thorough document. Then they published it for everyone. I think it taught a whole generation of designers and developers how to build great mobile apps, they would just read that document. There is a Human Interface Guideline for the Vision Pro and one of the things you notice is so much of it is about eye tracking, and communicating information with depth and space.

I think what brings maybe this is actually something for founders to think about if you're building an app into space, is that the Vision Pro they invested so much in eye tracking to make it work for so many reasons. I mean, we talked about to get just the rendering to work, that was a building block, but for the UX it is the moment that we're seeing with capacitive touch where Apple got it right for the iPhone. Eye tracking is starting to look a lot like that, so I think there's a lot of cool UX things that are yet to be discovered with just eye tracking. And the funny thing is, that the VR community was very skeptical of this because actually, it was actually a bad practice to do eye tracking because it tires the user too much and the reason was that the hardware was not good enough. I remember the same thing before the iPhone came out. I remember like a lot of the conventional wisdom from consultants and experts was that the virtual keyboard wouldn't work, that people wanted a physical keyboard, and that just, it wouldn't, like people would never treat it as like a serious device to do their email on because it didn't have a real keyboard on the phone.

Oh yeah, yeah, yeah, that was all the reviews of the iPhone, but there were, I mean this is maybe where founders should sort of pay attention, there were still things that Apple had not figured out yet, that third-party developers ended up figuring out. So if you remember, the pull down to refresh that was something that I think was in a Twitter client and you know, that founder ended up selling their Twitter client to Twitter and working at Twitter for a while. There are all kinds of new interactions that I think we have not figured out yet like this sort of pinch to move around is merely the first of a whole bunch of different things that frankly, end-user developers will actually figure out. 

I'm curious also, Diana, what's the difference for a developer between the Meta SDK and the Apple Vision Pro SDK? One of the big ones is Meta comes from the DNA of gaming, so they have very good support for Unity and Unreal, and those are game engines which are cool to build for games 3D environments in a game, which are literally more like a constrained 3D world. But for spatial or spatial computation, the real world is infinite, so sometimes game engines don't quite fit. One of the things you'll notice, to build an application that opens a PDF for the Meta platform, it actually takes a lot of lines of code, whereas to build that for the Vision OS, it's actually just a few lines of code interesting. 

I guess the other big question that probably a lot of people in the community have is this an iPhone moment or a Newton moment? Well, when the iPhone first launched, there wasn't actually an App Store, right? So I think that came maybe a year later something like that. All of the initial apps that got distribution on the App Store were like frivolous apps right, it's like the fart app, there's like a bunch of things that were getting really popular, the $2000 I am Rich app yeah, it's like an image of a ruby or something, yeah oh my god. And if you think about from our like at least the YC perspective, the iPhone or mobile didn't start driving really big companies being started until I would say probably like 2012. Like 2012 is the year where we had Instacart come through. I actually think mobile was a fairly big component of Coinbase right? Like they had the fact that they just had an easy to use mobile app. Doordash was 2013. And so all of these things started and of course, you had the rise of Uber not YC company but it took, so you could say five years from the launch of the iPhone for the actual good companies to even be founded.

And so yeah, you haven't missed it yet, well I don't, when I think about the Vision Pro, I'm not sure if we're at like, is this the iPhone moment in the sense of the iPhone just got launched and like it's still going to be a few years or is this like hey actually like this is, this device has been around for a while this is just the iteration that was needed, on it to unlock like the Instacarts and Door Dashes and Ubers that are going to be built on it. I’ll give one argument for why it's probably more like the iPhone moment we don't know. 

But you know when the iPhone came out, people forget smartphones were already an established category and the iPhone was like the new entrant to this established category a lot of people were skeptical that Apple could actually execute. As you mentioned, people were very skeptical of the iPhone as the right product to challenge the Blackberry and the other incumbent smartphones. It's like the famous Steve Ballmer quote about, I think there was like Steve Ballmer just, you know, making fun of it, and saying it would never be a serious device. That's right. That's right. 

Why was it that it took like five years for the good iPhone companies to come out? I think adoption had to happen, so that's why it actually maps very closely. I mean I don't know how many Apple actually sold, but it's probably on the order of hundreds of thousands, right? Which probably mirrors the iPhone; maybe the iPhone broke.

Even when you look back to the Instacart, DoorDash, or Uber moment, these mobile workforces could only happen when 70 to 80 percent of people in society had these devices. The reason why that was such an important moment was that it was the first time normal, average people had always-on internet connectivity and an app ecosystem that was actually stable enough. You know, remember back, you know, sort of 10 years before, it was like J2ME or do we write it in Flash? You know, Gustav and his you know, Voxer expansion experience. The platforms were literally so broken and so fragmented that you couldn't have 80% of the population on one platform, and then suddenly all of the platforms sort of coalesced, and then it opened up the market.

I guess the question with this device, and in general with VR, it will be different than mobile. It won't be a type of device perhaps, I mean, it depends on the price point. When it gets to maybe phone cost perhaps, but it will take a lot of time before we get that level of mass adoption. But I think what could happen is it will capture a lot of the kind of high-end use with what we talked about earlier with high information density construction CAD engineering type of workflows.

So, Diana and I were actually doing group office hours yesterday with a group of our company in this current batch who are working with hardware, hard tech ideas. We did this exercise we call it the pre-mortem where you sort of give them different flavors of how companies can die, and you get them to say this is like how I think I'm most likely to die. The one I'm coming up with, the thing that springs to mind here is we were talking about how Tesla's strategy was very successful to launch the Roadster, like a very high-end device, and then you bring out like the Model S and the Model 3 and the Model Y. But like that wouldn't have worked if they just stuck with the Roadster, right? And so maybe one failure mode for the Vision Pro is like this is the Tesla Roadster. It's great, it carves out like a niche for people who are really into this stuff and are willing to pay for a very high-end device, but I can't follow it up with like the Model 3. I think this is a bit of a chicken and egg aspect with it.

Because for this to be relevant, to become the Model 3, let's say, we need an ecosystem of applications and an incentive for developers to work on it. Because if I were a founder right now and I'm looking for a new idea, do I want to put all my eggs in here when there's not enough user yet? When should I do it? Should I just take a leap of faith on how do we advise founders when they're in this space? Why should they do it? I definitely think that's relevant to like the Instacart, DoorDash thing for example. If you think about it, those companies weren't making a bigger bet. Their apps were not specific to iOS or Apple, right? Like, everybody had a device, they worked equally well on Android, they frankly could have just been a web view stuffed in an app, right?

And they also weren't the first entrants in their categories. Like before DoorDash and Instacart, there were many would-be DoorDash and Instacart players that launched earlier that actually didn't succeed. Yeah, well, even more extreme, like, they, in their case, mobile actually made ideas that seemed very bad, like good ones. I actually think it's really cool that Sequoia invested in Instacart because they'd had the big failure with like Webvan, and so they had all this egg on their face with like grocery deliveries, this bad idea that you would expect is very natural to never want to fund that again. But like, mobile actually turned that into a good idea.

I did a dinner talk with Max, the co-founder of Instacart, and he said that when Sequoia led the Series A for Instacart they gave him the Webvan business plan that they had been given in the 90s. But the problem was it was on a floppy disk and he couldn't find a floppy disk reader, so he never read it. That's hilarious.

I'm sort of taken by even the path of consumer social networks. You know, Facebook started as the blue app. It was a desktop experience killing MySpace. It sort of looked like literally bank software. If you logged into Facebook or Chase.com, it even had the same color. I remember being at YC when Mark Zuckerberg came to talk about why they bought Oculus, and it was actually very much from what I could tell, trying to fight the last war. Facebook had this monopoly. It had owned the industry of consumer social, but then they almost lost it because Instagram easily could have outstripped it. That was because of a platform shift.  So he wanted to clearly own the next platform, and he's right.

Should founders go build on this? Is this a good opportunity for startups? I just sort of wonder what things could actually fully take advantage of this in a real professional context. I mean, where my head goes, maybe it's too obvious, but traders with their 20 screens, wouldn't you rather have something that allowed you to take in the breadth of that information and dive into it very easily just by going like that? You can imagine that being something that people are actually willing to pay not just hundreds of dollars a month but maybe thousands of dollars a month for.

I think we're going to be in not quite some time at the beginning in this awkward phase with spatial computing-type apps because even with the Apple SDK and Meta, a lot of things are still flat 2D, and I don't think we know how to develop or develop for full 3D, what really truly takes advantage of this platform, what is unique about this platform, whether it's 360-degree view, being able to dive into more data easily. What are aspects of this new technology that mean that it can upend even what seems like an unassailable incumbent like Snapchat versus Facebook? But would part of you try and talk them out of it, like would part of you be thinking this is too early, you should work on something else and not?

I think if you look back in our history, YC has really been pretty good at this where every time there's a platform shift whether it's like the Facebook thing which didn't go anywhere or the iOS thing which did go places, we were reasonably accurate actually funding the right stuff. And I think the way that we did it is rather than having a strong thesis on each technology and each platform, we just kind of look at each application from first principles and we talk to the founders and they have some idea. We just try to figure out if the idea makes sense. I think that's what allows us to have had a pretty good track record of discriminating people who are just like cargo culting the new thing and just like jumping on the hype train and have some idea that doesn't really make sense from the people who are building something like DoorDash that actually totally makes sense.

Yeah, that's fine. The other thing that I would look at to Jared's point is actually there's a strong belief from the founder that they want to make a bet in this space. I think there's just something about founders where they go all in, they become unstoppable, and it's going to take time so they have to have the faith that this is going to be different than building, let's say, a standard SaaS application or consumer app or AI application. If you stick long enough, you're going to build a lot of expertise and be world-class by the time is the right moment. But someone that's genuinely excited about it, and the cool thing about it, there's a lot of technical challenges with it which I think is going to attract the right kind of founders because it's actually hard to build something good on this right now because it's so new. So this will be the main thing I'll look for when I'm reading applications for people putting VR stuff, actually.

And I feel okay sharing it because it's very hard to fake. It's basically what we're saying is if you're the kind of person that just is irrationally compelled to build applications for VR, we will happily fund you, and like we need some evidence of that because you just like, spend your spare in your free time, you are like building VR apps and you have been for a while. Like yeah, we never try and discourage founders from building stuff they just think is cool.

Well, that's a great place to end. We're out of time, but thank you guys, another good episode of the Light Cone. Guys, see you next time. Catch you guys next.


You.
You.
You.
You.

